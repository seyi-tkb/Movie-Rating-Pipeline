{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca0d3b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (37823289.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    -----\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config.py\n",
    "-----\n",
    "# import needed library\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# dictionary of urls to read data\n",
    "urls = {\n",
    "    \"movies\": \"https://drive.google.com/file/d/188tIKLJKek62rGmzj1Ylc03fe4Pgb5co/view?usp=drive_link\",\n",
    "    \"ratings\": \"https://drive.google.com/file/d/1-3S-XOgZyo9D3sVoXtjPvmFdsihjfQhN/view?usp=drive_link\",\n",
    "    \"users\": \"https://drive.google.com/file/d/1_wAww5beF2K7dpx-SU_gUUddNWeaeZqv/view?usp=drive_link\"\n",
    "}\n",
    "\n",
    "# dictionary of file columns\n",
    "expected_columns = {\n",
    "    \"movies\": ['item_id', 'movie_title', 'release_date', 'IMDb_URL', 'primary_genre'],\n",
    "    \"ratings\": ['user_id', 'item_id', 'rating', 'timestamp'],\n",
    "    \"users\": ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "}\n",
    "\n",
    "# date columns to process\n",
    "date_columns = {\n",
    "    \"movies\": \"release_date\",\n",
    "    \"ratings\": \"timestamp\"\n",
    "}\n",
    "\n",
    "# AWS S3 configuration keys\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "S3_BUCKET_BRONZE = os.getenv(\"S3_BUCKET_BRONZE\")\n",
    "S3_BUCKET_SILVER = os.getenv(\"S3_BUCKET_SILVER\")\n",
    "S3_BUCKET_GOLD = os.getenv(\"S3_BUCKET_GOLD\")\n",
    "WATERMARKS_PATH = \"watermarks/watermarks.parquet\"  # JSON preferred over parquet for metadata\n",
    "\n",
    "# Postgres configuration keys\n",
    "POSTGRES_CONFIG = {\n",
    "    \"host\": os.getenv(\"POSTGRES_HOST\", \"localhost\"),\n",
    "    \"port\": os.getenv(\"POSTGRES_PORT\", \"5432\"),\n",
    "    \"database\": os.getenv(\"POSTGRES_DB\", \"movie_rating_database\"),\n",
    "    \"user\": os.getenv(\"POSTGRES_USER\"),\n",
    "    \"password\": os.getenv(\"POSTGRES_PASSWORD\")\n",
    "}\n",
    "\n",
    "# orchestration\n",
    "pipeline_schedule = {\n",
    "    \"frequency\": \"weekly\",  # can be \"daily\", \"weekly\", etc.\n",
    "    \"day_of_week\": \"sunday\"  # if weekly\n",
    "}\n",
    "\n",
    "\n",
    "utils/utils.py\n",
    "-----\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_logger(name: str, log_level=logging.INFO) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Creates and returns a logger instance with console and file handlers.\n",
    "    \n",
    "    Args:\n",
    "        name (str): The name of the logger, __name__.\n",
    "        log_level (int): Logging level, default is INFO.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a logs folder if it doesn't exist\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "    # define the log filename with timestamp\n",
    "    log_filename = f\"logs/{datetime.now().strftime('%Y-%m-%d')}.log\"\n",
    "\n",
    "    # create a logger\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(log_level)\n",
    "\n",
    "    # prevent duplicating of logging\n",
    "    if not logger.handlers:\n",
    "        \n",
    "        # Create formatter\n",
    "        formatter = logging.Formatter(\n",
    "            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "        )\n",
    "\n",
    "        # Console Handler\n",
    "        console_handler = logging.StreamHandler()       \n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "        # File Handler\n",
    "        file_handler = logging.FileHandler(log_filename)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "s3_client.py\n",
    "----------\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from botocore.exceptions import ClientError\n",
    "from config import AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION\n",
    "from utils.utils import get_logger\n",
    "\n",
    "# Initialize Logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def initialize_s3_client():\n",
    "    \"\"\"\n",
    "    Initialize and return a boto3 S3 client instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "            region_name=AWS_REGION,\n",
    "        )\n",
    "        logger.info(\"Successfully initialized S3 client.\")\n",
    "        return client\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize S3 client: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_bucket(client, bucket_name):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket if it doesn't exist already.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if bucket exists\n",
    "        existing_buckets = client.list_buckets()\n",
    "        if not any(b[\"Name\"] == bucket_name for b in existing_buckets[\"Buckets\"]):\n",
    "            client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={\"LocationConstraint\": AWS_REGION},\n",
    "            )\n",
    "            logger.info(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "        else:\n",
    "            logger.info(f\"Bucket '{bucket_name}' already exists.\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Bucket creation error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def upload_file(client, bucket_name, object_name, file_path):\n",
    "    \"\"\"\n",
    "    Upload a file to a specified S3 bucket.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.upload_file(file_path, bucket_name, object_name)\n",
    "        logger.info(\n",
    "            f\"File '{file_path}' uploaded to bucket '{bucket_name}' as '{object_name}'.\"\n",
    "        )\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Upload error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def download_file(client, bucket_name, object_name, file_path):\n",
    "    \"\"\"\n",
    "    Download a file from S3 bucket to local path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.download_file(bucket_name, object_name, file_path)\n",
    "        logger.info(\n",
    "            f\"File '{object_name}' downloaded from bucket '{bucket_name}' to '{file_path}'.\"\n",
    "        )\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Download error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def read_file(client, bucket_name, object_name):\n",
    "    \"\"\"\n",
    "    Read a file (CSV/Parquet) from S3 bucket into a DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.get_object(Bucket=bucket_name, Key=object_name)\n",
    "        logger.info(f\"Reading file '{object_name}' from bucket '{bucket_name}'.\")\n",
    "\n",
    "        # Guess file type from extension\n",
    "        if object_name.endswith(\".parquet\"):\n",
    "            df = pd.read_parquet(BytesIO(response[\"Body\"].read()))\n",
    "        else:\n",
    "            df = pd.read_csv(BytesIO(response[\"Body\"].read()))\n",
    "\n",
    "        logger.info(f\"File '{object_name}' successfully read into DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Read error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "bronze/ingest.py\n",
    "-----------\n",
    "# importing libraries/modules\n",
    "import pandas as pd\n",
    "from utils.utils import get_logger\n",
    "\n",
    "# initialize logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# function to read files from source (drive)\n",
    "def read_files(urls: dict) -> dict: \n",
    "    \"\"\"\n",
    "    Reads a list of the 3 file paths into dataframes\n",
    "\n",
    "    Parameters:\n",
    "        urls: dicyionary of file names and their urls\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of file names and created dataframes\n",
    "    \"\"\"\n",
    "    dataframes = {}\n",
    "\n",
    "    for file, url in urls.items():\n",
    "        try:\n",
    "            file_id = url.split('/')[-2]\n",
    "            direct_url = f\"https://drive.usercontent.google.com/download?id={file_id}&export=download&authuser=0&confirm=t\"\n",
    "            \n",
    "            logger.info(f\"Reading {file}...\")\n",
    "            \n",
    "            df = pd.read_csv(direct_url)\n",
    "            dataframes[file] = df\n",
    "            \n",
    "            logger.info(f\"Successfully read {file}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Failed to read {file}. Error: {e}\")\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "\n",
    "\n",
    "bronze/validation.py\n",
    "-----------\n",
    "# importing libraries/modules\n",
    "from utils.utils import get_logger\n",
    "\n",
    "# initialize logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# function to check columns\n",
    "def validate_columns(dataframes: dict, expected_cols: dict):\n",
    "    \"\"\"\n",
    "    Validates that each dataframe contains the expected columns.\n",
    "    Args:\n",
    "        dataframes (dict): {name: dataframe}\n",
    "        expected_columns (dict): {name: list of expected columns}\n",
    "    \"\"\"\n",
    "    for file, df in dataframes.items():\n",
    "        logger.info(f\"Validating columns for {file}..\")\n",
    "        expected_columns = set(expected_cols[file])\n",
    "        actual_columns = set(df.columns)\n",
    "        missing = expected_columns - actual_columns\n",
    "        extra = actual_columns - expected_columns\n",
    "\n",
    "        # if missing set is not empty\n",
    "        if missing: \n",
    "            logger.error(f\"Missing columns in {file}: {missing}\")\n",
    "            raise ValueError(f\"Missing columns in {file}: {missing}\")\n",
    "        \n",
    "        # if extra columns are present\n",
    "        if extra:\n",
    "            logger.warning(f\"Extra columns found in {file}: {extra}\")\n",
    "\n",
    "        logger.info(f\"successfully validated {file}'s columns.\")\n",
    "\n",
    "# function to check null values\n",
    "def validate_nulls(dataframes: dict):\n",
    "    \"\"\"\n",
    "    Logs if there are any null values in the dataframes.\n",
    "    Args:\n",
    "        dataframes (dict): {name: dataframe}\n",
    "    \"\"\"\n",
    "    for name, df in dataframes.items():\n",
    "        null_counts = df.isnull().sum()\n",
    "        total_nulls = null_counts.sum()\n",
    "        if total_nulls > 0:\n",
    "            logger.warning(f\"{name} has {total_nulls} missing values:\\n{null_counts[null_counts > 0]}\")\n",
    "        else:\n",
    "            logger.info(f\"No missing values found in {name}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "\n",
    "\n",
    "bronze/upload.py\n",
    "-----------\n",
    "# importing libraries/modules\n",
    "import io\n",
    "import boto3\n",
    "from utils.utils import get_logger\n",
    "from pipeline.s3_client import initialize_s3_client\n",
    "from config import S3_BUCKET_BRONZE\n",
    "\n",
    "# initialize logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def upload_to_bronze(dataframes: dict):\n",
    "    \"\"\"\n",
    "    Uploads dataframes to a specified S3 Bronze bucket.\n",
    "\n",
    "    Parameters:\n",
    "        dataframes: dictionary of file names and their dataframes.\n",
    "    \"\"\"\n",
    "    s3_client = initialize_s3_client()\n",
    "\n",
    "    for file, df in dataframes.items():\n",
    "        try:\n",
    "            logger.info(f\"Starting {file} upload to bronze bucket\")\n",
    "\n",
    "            # Convert DataFrame to in-memory buffer\n",
    "            csv_buffer = io.StringIO()\n",
    "            df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "            object_name = f\"{file}.csv\"  # prefix ensures Bronze folder in bucket\n",
    "            logger.info(f\"Uploading {object_name} to S3 bucket {S3_BUCKET_BRONZE}\")\n",
    "\n",
    "            # Upload the file to S3\n",
    "            s3_client.put_object(\n",
    "                Bucket=S3_BUCKET_BRONZE,\n",
    "                Key=object_name,\n",
    "                Body=csv_buffer.getvalue()\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Successfully uploaded {object_name} to S3 bucket {S3_BUCKET_BRONZE}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to upload {file}. Error: {e}\")\n",
    "\n",
    "\n",
    "bronze/orchestration.py\n",
    "-------------\n",
    "# Importing required modules\n",
    "from pipeline.a_bronze.ingest import read_files\n",
    "from pipeline.a_bronze.validation import validate_columns, validate_nulls\n",
    "from pipeline.a_bronze.upload import upload_to_bronze\n",
    "from utils.utils import get_logger\n",
    "from config import urls, expected_columns, S3_BUCKET_BRONZE\n",
    "from pipeline.s3_client import initialize_s3_client, create_bucket\n",
    "\n",
    "# Initialize logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Initialize AWS S3 client\n",
    "client = initialize_s3_client()\n",
    "\n",
    "def source_to_bronze():\n",
    "    \"\"\"Main function to orchestrate the data pipeline with error handling.\"\"\"\n",
    "    \n",
    "    logger.info(\"Starting Bronze layer data pipeline...\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Read raw files\n",
    "        dataframes = read_files(urls)\n",
    "        logger.info(\"Successfully ingested raw data.\")\n",
    "\n",
    "        # Step 2: Validate schema & nulls\n",
    "        validate_columns(dataframes, expected_columns)\n",
    "        validate_nulls(dataframes)\n",
    "        logger.info(\"Data validation completed successfully.\")\n",
    "\n",
    "        # Step 3: Ensure Bronze bucket exists\n",
    "        create_bucket(client, S3_BUCKET_BRONZE)\n",
    "\n",
    "        # Step 4: Upload to Bronze\n",
    "        upload_to_bronze(dataframes)\n",
    "        logger.info(\"Data successfully uploaded to Bronze S3 bucket.\")\n",
    "\n",
    "        logger.info(\"Bronze pipeline execution completed successfully!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline execution failed. Error: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_to_bronze()\n",
    "\n",
    "\n",
    "\n",
    "b_silver/read_write_buckets.py\n",
    "---------------\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pipeline.s3_client import initialize_s3_client\n",
    "from utils.utils import get_logger\n",
    "from config import S3_BUCKET_BRONZE, S3_BUCKET_SILVER\n",
    "\n",
    "# Initialize Logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Initialize AWS S3 Client\n",
    "client = initialize_s3_client()\n",
    "\n",
    "\n",
    "def read_bronze_file(object_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the Bronze bucket into a DataFrame.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Reading {object_name} from {S3_BUCKET_BRONZE} bucket.\")\n",
    "    try:\n",
    "        response = client.get_object(Bucket=S3_BUCKET_BRONZE, Key=object_name)\n",
    "        data = response[\"Body\"].read()\n",
    "        # bytesIO makes it so i dont have to download the file. \n",
    "        # get to read the content from memory\n",
    "        # masks the binary content and tricks ps.r_csv to think it is a real file\n",
    "        df = pd.read_csv(BytesIO(data)) \n",
    "        logger.info(f\"Successfully read {object_name} into DataFrame. Shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading {object_name} from S3 Bronze bucket: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def write_to_silver(df: pd.DataFrame, object_name: str):\n",
    "    \"\"\"\n",
    "    Uploads a DataFrame as a CSV file into the Silver bucket.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Writing {object_name} to {S3_BUCKET_SILVER} bucket.\")\n",
    "    try:\n",
    "        csv_buffer = BytesIO()\n",
    "        df.to_csv(csv_buffer, index=False) # puts content inside buffer\n",
    "        csv_buffer.seek(0) # resets buffer pointer to line 1 (beginning)\n",
    "\n",
    "        client.put_object(\n",
    "            Bucket=S3_BUCKET_SILVER,\n",
    "            Key=object_name,\n",
    "            Body=csv_buffer.getvalue(),\n",
    "            ContentType=\"application/csv\"\n",
    "        )\n",
    "        logger.info(f\"Successfully wrote {object_name} to Silver bucket.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing {object_name} to Silver bucket: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "\n",
    "\n",
    "b_silver/watermarks.py\n",
    "------------\n",
    "import pandas as pd\n",
    "import io\n",
    "from pipeline.s3_client import initialize_s3_client\n",
    "from config import S3_BUCKET_BRONZE, WATERMARKS_PATH\n",
    "from utils.utils import get_logger\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# initialize logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# initialize S3 client\n",
    "client = initialize_s3_client()\n",
    "\n",
    "# function to read watermarks\n",
    "def read_watermarks():\n",
    "    \"\"\"\n",
    "    Reads the existing watermarks from S3.\n",
    "    Returns an empty DataFrame if no watermark exists.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Reading watermarks from S3 Bronze bucket...\")\n",
    "        response = client.get_object(Bucket=S3_BUCKET_BRONZE, Key=WATERMARKS_PATH)\n",
    "\n",
    "        # Read parquet file directly from S3 response\n",
    "        watermarks_df = pd.read_csv(io.BytesIO(response[\"Body\"].read()))\n",
    "        logger.info(\"Successfully read existing watermarks.\")\n",
    "        return watermarks_df\n",
    "\n",
    "    except ClientError as e:\n",
    "        # If the file doesn't exist, return empty DataFrame\n",
    "        logger.warning(f\"Watermarks not found in S3. Initializing new one. Details: {e}\")\n",
    "        columns = [\"dataset_name\", \"max_value\", \"records_loaded\", \"processing_time\"]\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "# function to initialize watermarks\n",
    "def initialize_watermarks(initial_watermarks: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Initializes the watermarks table during the first load.\n",
    "    \"\"\"\n",
    "    logger.info(\"Initializing watermarks table in S3...\")\n",
    "    try:\n",
    "        buffer = io.BytesIO()\n",
    "        initial_watermarks.to_csv(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "\n",
    "        client.put_object(\n",
    "            Bucket=S3_BUCKET_BRONZE,\n",
    "            Key=WATERMARKS_PATH,\n",
    "            Body=buffer.getvalue(),\n",
    "            ContentType=\"application/csv\"\n",
    "        )\n",
    "        logger.info(\"Successfully initialized watermark in S3.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize watermarks: {e}\")\n",
    "        raise\n",
    "\n",
    "# function to update watermarks\n",
    "def update_watermarks(new_watermark: dict):\n",
    "    \"\"\"\n",
    "    Updates the watermarks table after an incremental load.\n",
    "    \"\"\"\n",
    "    logger.info(\"Updating watermarks in S3...\")\n",
    "    try:\n",
    "        # read old watermarks if available\n",
    "        try:\n",
    "            old_watermarks = read_watermarks()\n",
    "        except Exception:\n",
    "            logger.warning(\"No existing watermark file. Creating a new one.\")\n",
    "            old_watermarks = pd.DataFrame(columns=[\"dataset_name\", \"max_value\", \"records_loaded\", \"processing_time\"])\n",
    "\n",
    "        # add the new row\n",
    "        new_row = pd.DataFrame([new_watermark])\n",
    "        updated_watermark = pd.concat([old_watermarks, new_row], ignore_index=True)\n",
    "\n",
    "        # write back to S3\n",
    "        buffer = io.BytesIO()\n",
    "        updated_watermark.to_csv(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "\n",
    "        client.put_object(\n",
    "            Bucket=S3_BUCKET_BRONZE,\n",
    "            Key=WATERMARKS_PATH,\n",
    "            Body=buffer.getvalue(),\n",
    "            ContentType=\"application/csv\"\n",
    "        )\n",
    "        logger.info(\"Watermarks updated successfully in S3.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to update watermarks: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "\n",
    "\n",
    "b_silver/transform.py\n",
    "-------------\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pipeline.s3_client import initialize_s3_client\n",
    "from utils.utils import get_logger\n",
    "from pipeline.b_silver.watermarks import read_watermarks, update_watermarks\n",
    "from pipeline.b_silver.read_write_buckets import read_bronze_file, write_to_silver\n",
    "\n",
    "# initialize Logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# initialize s3 Client\n",
    "client = initialize_s3_client()\n",
    "\n",
    "\n",
    "# function to tranform movie df\n",
    "def prepare_movie_df():\n",
    "    \"\"\"\n",
    "    Clean the movie data\n",
    "    \"\"\"\n",
    "    # read from bronze \n",
    "    logger.info(\"Starting transformations for movie data..\")\n",
    "    df = read_bronze_file('movies.csv')\n",
    "\n",
    "    # standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # ensure release_date is datetime\n",
    "    df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
    "\n",
    "    # drop rows with missing item_id or movie_title\n",
    "    df = df.dropna(subset=['item_id', 'movie_title'])\n",
    "\n",
    "    # trim and clean string columns\n",
    "    df['movie_title'] = df['movie_title'].str.strip()\n",
    "    df['primary_genre'] = df['primary_genre'].str.strip().str.title()\n",
    "    df['imdb_url'] = df['imdb_url'].str.strip()\n",
    "\n",
    "    # final shape logging\n",
    "    logger.info(f\"Movies data cleaned with {df.shape[0]} records\")\n",
    "\n",
    "    # incremental filter based on watermark\n",
    "    watermarks = read_watermarks()\n",
    "\n",
    "    if not watermarks.empty:\n",
    "        logger.info(\"Applying watermark filter for movies..\")\n",
    "        \n",
    "        # Get latest max_value for the 'ratings' dataset\n",
    "        movie_watermark = watermarks[watermarks[\"dataset_name\"] == \"movies\"]\n",
    "                               \n",
    "        if not movie_watermark.empty:\n",
    "            latest_watermark = pd.to_datetime(movie_watermark['max_value'].max())\n",
    "            df = df[df['release_date'] > latest_watermark]\n",
    "            logger.info(f\"Filtered movie_df to {df.shape[0]} new records after watermark {latest_watermark}.\")\n",
    "        else:\n",
    "            logger.warning(\"No existing watermark for movies. Proceeding without filter.\")\n",
    "    else:\n",
    "        logger.warning(\"No watermark file found. Proceeding without filter.\")\n",
    "\n",
    "    # warning if no new data\n",
    "    if df.empty:\n",
    "        logger.warning(\"No new movie data to process after watermark filtering.\")\n",
    "    else:\n",
    "        logger.info(f\"Movie data cleaned and filtered with {df.shape[0]} records.\")\n",
    "    \n",
    "    # upload to silver\n",
    "    write_to_silver(df, 'movies.csv')\n",
    "\n",
    "    # update the watermark with the latest processing record\n",
    "    if not df.empty:\n",
    "        latest_max_value = df['release_date'].max()\n",
    "        data = {'dataset_name': 'movies',\n",
    "                'max_value': latest_max_value,\n",
    "                'records_loaded': df.shape[0],\n",
    "                'processing_time': pd.Timestamp.now()\n",
    "                                }\n",
    "        \n",
    "        update_watermarks(data)\n",
    "\n",
    "\n",
    "# function to transform ratings df\n",
    "def prepare_ratings_df():\n",
    "    \"\"\"\n",
    "    Clean the ratings data\n",
    "    \"\"\"\n",
    "    # read from bronze \n",
    "    df = read_bronze_file('ratings.csv')\n",
    "\n",
    "    logger.info(\"Starting transformations for ratings..\")\n",
    "\n",
    "    # standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "    # Drop exact duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Drop rows with missing user_id, item_id or rating\n",
    "    df = df.dropna(subset=['user_id', 'item_id', 'rating'])\n",
    "\n",
    "    # Convert timestamp into proper datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "\n",
    "    logger.info(f\"Ratings data cleaned with {df.shape[0]} records\")\n",
    "\n",
    "    # incremental filter based on watermark\n",
    "    watermarks = read_watermarks()\n",
    "\n",
    "    if not watermarks.empty:\n",
    "        logger.info(\"Applying watermark filter for ratings..\")\n",
    "        \n",
    "        # Get latest max_value for the 'ratings' dataset\n",
    "        ratings_watermark = watermarks[watermarks[\"dataset_name\"] == \"ratings\"]\n",
    "                               \n",
    "        if not ratings_watermark.empty:\n",
    "            latest_watermark = pd.to_datetime(ratings_watermark['max_value'].max(), unit='s')\n",
    "            df = df[df['timestamp'] > latest_watermark]\n",
    "            logger.info(f\"Filtered ratings to {df.shape[0]} new records after watermark {latest_watermark}.\")\n",
    "        else:\n",
    "            logger.warning(\"No existing watermark for ratings. Proceeding without filter.\")\n",
    "    else:\n",
    "        logger.warning(\"No watermark file found. Proceeding without filter.\")\n",
    "\n",
    "    if df.empty:\n",
    "        logger.warning(\"No new ratings data to process after watermark filtering.\")\n",
    "    else:\n",
    "        logger.info(f\"Ratings data cleaned and filtered with {df.shape[0]} records.\")\n",
    "\n",
    "    # upload to silver\n",
    "    write_to_silver(df, 'ratings.csv')\n",
    "\n",
    "    # update the watermark with the new details\n",
    "    if not df.empty:\n",
    "        latest_max_value = df['timestamp'].max()\n",
    "        data = {'dataset_name': 'ratings',\n",
    "                'max_value': latest_max_value,\n",
    "                'records_loaded': df.shape[0],\n",
    "                'processing_time': pd.Timestamp.now()\n",
    "                                }\n",
    "        \n",
    "        update_watermarks(data)\n",
    "\n",
    "\n",
    "# function to transform user df\n",
    "def prepare_users_df():\n",
    "    \"\"\"\n",
    "    Clean the user data\n",
    "    \"\"\"\n",
    "    \n",
    "    # read from bronze \n",
    "    df = read_bronze_file('users.csv')\n",
    "\n",
    "    logger.info(\"Starting transformations for users..\")\n",
    "\n",
    "    # standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    # drop users without user_id\n",
    "    df = df.dropna(subset=['user_id'])\n",
    "\n",
    "    # drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # clean string fields: whitespaces and case\n",
    "    df['gender'] = df['gender'].str.strip().str.upper()\n",
    "    df['occupation'] = df['occupation'].str.strip().str.title()\n",
    "    df['zip_code'] = df['zip_code'].astype(str).str.strip()\n",
    "\n",
    "    logger.info(f\"Users data cleaned with {df.shape[0]} records\")\n",
    "\n",
    "    # incremental filter based on watermark\n",
    "    watermarks = read_watermarks()\n",
    "\n",
    "    if not watermarks.empty:\n",
    "        logger.info(\"Applying watermark filter for users..\")\n",
    "\n",
    "        # Get latest max_value for the 'users' dataset\n",
    "        user_watermark = watermarks[watermarks[\"dataset_name\"] == \"users\"]\n",
    "\n",
    "        if not user_watermark.empty:\n",
    "            latest_watermark = int(user_watermark[\"max_value\"].max())\n",
    "            df = df[df[\"user_id\"].astype(int) > latest_watermark]\n",
    "            logger.info(f\"Filtered user data to {df.shape[0]} new records after user_id {latest_watermark}.\")\n",
    "        \n",
    "        else:\n",
    "            logger.warning(\"No existing watermark for ratings. Proceeding without filter.\")\n",
    "    else:\n",
    "        logger.warning(\"No watermark file found. Proceeding without filter.\")\n",
    "\n",
    "    if df.empty:\n",
    "        logger.warning(\"No new user data to process after watermark filtering.\")\n",
    "    else:\n",
    "        logger.info(f\"User data cleaned and filtered with {df.shape[0]} records.\")\n",
    "    \n",
    "    # upload to silver\n",
    "    write_to_silver(df, 'users.csv')\n",
    "\n",
    "    # update the watermark with the new details\n",
    "    if not df.empty:\n",
    "        latest_max_value = df['user_id'].max()\n",
    "        data = {'dataset_name': 'users',\n",
    "                'max_value': latest_max_value,\n",
    "                'records_loaded': df.shape[0],\n",
    "                'processing_time': pd.Timestamp.now()\n",
    "                                }\n",
    "        \n",
    "        update_watermarks(data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "\n",
    "\n",
    "b_siver/orchestrator_B-S.py\n",
    "---------------\n",
    "from utils.utils import get_logger\n",
    "from pipeline.b_silver.transform import prepare_movie_df, prepare_ratings_df, prepare_users_df\n",
    "\n",
    "# need\n",
    "\n",
    "# initializing logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def orchestrate():\n",
    "    logger.info(\"Starting bronze-silver orchestration...\")\n",
    "\n",
    "    try:\n",
    "        prepare_movie_df()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Movie pipeline failed: {e}\")\n",
    "\n",
    "    try:\n",
    "        prepare_ratings_df()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ratings pipeline failed: {e}\")\n",
    "\n",
    "    try:\n",
    "        prepare_users_df()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Users pipeline failed: {e}\")\n",
    "\n",
    "    logger.info(\"MovieLens ETL Orchestration Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    orchestrate()\n",
    "\n",
    "\n",
    "s_gold/read_bucket.py\n",
    "----------------\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pipeline.s3_client import initialize_s3_client\n",
    "from utils.utils import get_logger\n",
    "from config import S3_BUCKET_SILVER\n",
    "\n",
    "\n",
    "# initialize Logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# initialize s3 Client\n",
    "client = initialize_s3_client()\n",
    "\n",
    "\n",
    "# function to read from silver bucket\n",
    "def read_silver_file(object_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file from silver bucket into a DataFrame.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Reading {object_name} from {S3_BUCKET_SILVER} bucket.\")\n",
    "    try:\n",
    "        response = client.get_object(Bucket= S3_BUCKET_SILVER, key= object_name)\n",
    "        data = response[\"body\"].read()\n",
    "        df = pd.read_csv(BytesIO(data))\n",
    "        logger.info(f\"Successfully read {object_name} into DataFrame. Shape: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading {object_name} from {S3_BUCKET_SILVER}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "s_gold/connection.py\n",
    "--------------\n",
    "import logging\n",
    "import pandas as pd\n",
    "from config import POSTGRES_CONFIG\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# initializing logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#function to connect to database\n",
    "def get_db_connection():\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            f\"postgresql://{POSTGRES_CONFIG['user']}:{POSTGRES_CONFIG['password']}@\"\n",
    "            f\"{POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}/{POSTGRES_CONFIG['database']}\"\n",
    "        )\n",
    "        logger.info(\"Successfully connected to Postgres.\")\n",
    "        return engine\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"PostgreSQL connection error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# function to create table\n",
    "def create_table_if_not_exists(sql: str, table: str):\n",
    "    try:\n",
    "        engine = get_db_connection()\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(sql)\n",
    "        logger.info(f\"Table '{table}' in Postgres.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating table '{table}': {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# function to upload to postgres\n",
    "def write_to_postgres(df: pd.DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Uploads DataFrame to Postgres.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Writing {table_name} to Postgres..\")\n",
    "\n",
    "    try:\n",
    "        engine = get_db_connection()\n",
    "        df.to_sql(name= table_name, con=engine, if_exists=\"append\", index=False, method='multi')\n",
    "\n",
    "        logger.info(f\"Successfully inserted {len(df)} records into {table_name}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing {table_name} df to PostgreSQL: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "s_gold/schema.py\n",
    "-------------\n",
    "\n",
    "# python\n",
    "movies_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movies (\n",
    "    item_id INT PRIMARY KEY,\n",
    "    movie_title TEXT NOT NULL,\n",
    "    release_date TIMESTAMP,\n",
    "    primary_genre TEXT,\n",
    "    imdb_url TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "users_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    user_id INT PRIMARY KEY,\n",
    "    gender CHAR(1),\n",
    "    age INT,\n",
    "    occupation TEXT,\n",
    "    zip_code TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "ratings_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ratings (\n",
    "    user_id INT REFERENCES users(user_id),\n",
    "    item_id INT REFERENCES movies(item_id),\n",
    "    rating DOUBLE PRECISION,\n",
    "    timestamp TIMESTAMP,\n",
    "    PRIMARY KEY (user_id, item_id, timestamp)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
